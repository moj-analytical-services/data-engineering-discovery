{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Use Cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the various data engineering use cases using Pandas logic. You have the option to read/write locally or to S3 by updating the relevant filepaths. The idea is to replicate these use cases using the different frameworks. We can then compare the code complexity for the different frameworks, as well as the code performance as the data volumes increase. You can run this notebook against either a standard python kernel locally or PySpark kernel with the Glue interactive session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Glue interactive session\n",
    "\n",
    "This section is optional, please skip when using a python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iam_role is arn:aws:iam::684969100054:role/aws-reserved/sso.amazonaws.com/eu-west-2/AWSReservedSSO_AdministratorAccess_ab408ccf26c25b37\n",
      "iam_role has been set to arn:aws:iam::684969100054:role/AdminAccessGlueNotebook.\n",
      "Previous region: eu-west-1\n",
      "Setting new region to: eu-west-1\n",
      "Reauthenticating Glue client with new region: eu-west-1\n",
      "IAM role has been set to arn:aws:iam::684969100054:role/AdminAccessGlueNotebook. Reauthenticating.\n",
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\n",
      "Authentication done.\n",
      "Region is set to: eu-west-1\n",
      "Setting session ID prefix to native-hudi-dataframe-\n",
      "Setting Glue version to: 3.0\n",
      "Current idle_timeout is 2880 minutes.\n",
      "idle_timeout has been set to 60 minutes.\n",
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: 5\n",
      "Setting new number of workers to: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following exception was encountered while parsing the configurations provided: invalid syntax (<unknown>, line 7) \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/soumaya.mauthoor/Documents/GitHub/hudi-vs-iceberg/venv/lib/python3.9/site-packages/aws_glue_interactive_sessions_kernel/glue_pyspark/GlueKernel.py\", line 444, in configure\n",
      "    configs = ast.literal_eval(configs_json)\n",
      "  File \"/Users/soumaya.mauthoor/.pyenv/versions/3.9.10/lib/python3.9/ast.py\", line 62, in literal_eval\n",
      "    node_or_string = parse(node_or_string, mode='eval')\n",
      "  File \"/Users/soumaya.mauthoor/.pyenv/versions/3.9.10/lib/python3.9/ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "  File \"<unknown>\", line 7\n",
      "    from awsglue.transforms import *\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%iam_role arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\n",
    "%region eu-west-1\n",
    "%session_id_prefix native-hudi-dataframe-\n",
    "%glue_version 3.0\n",
    "%idle_timeout 60\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "%%configure \n",
    "{\n",
    "  \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n",
    "  \"--datalake-formats\": \"hudi\"\n",
    "}\n",
    "\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import a python script, first upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./pandas_functions.py to s3://sb-test-bucket-ireland/data-engineering-use-cases/pandas_functions.py\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp pandas_functions.py s3://sb-test-bucket-ireland/data-engineering-use-cases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: 3ff9dea8-3101-454d-a4bd-9c4ecc20f49f\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.37.4\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session 3ff9dea8-3101-454d-a4bd-9c4ecc20f49f to get into ready status...\n",
      "Session 3ff9dea8-3101-454d-a4bd-9c4ecc20f49f has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.addPyFile(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/pandas_functions.py\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python libraries and set variables\n",
    "\n",
    "This section is not optional. Please update as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time, datetime\n",
    "from pandas_functions import bulk_insert, scd2_simple, scd2_complex\n",
    "\n",
    "future_end_datetime = datetime.datetime(2250, 1, 1)\n",
    "input_data_directory = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/\"\n",
    "full_load_filepath = f'{input_data_directory}full_load.parquet'\n",
    "updates_filepath = f'{input_data_directory}updates.parquet'\n",
    "late_updates_filepath = f'{input_data_directory}late_updates.parquet'\n",
    "output_data_directory = \"s3://sb-test-bucket-ireland/soumaya/de-usecases/pandas/pandas-pyspark/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple process which appends various columns to the full load data and saves it to a parquet file.\n",
    "\n",
    "1. Set `start_datetime` to `extraction_timestamp`\n",
    "2. Set `end_datetime` to a future distant timestamp\n",
    "3. Set `is_current` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product_id product_name  price extraction_timestamp    op\n",
      "0      00001       Heater    250  2022-01-01 01:01:01  None\n",
      "1      00002   Thermostat    400  2022-01-01 01:01:01  None\n",
      "2      00003   Television    600  2022-01-01 01:01:01  None\n",
      "3      00004      Blender    100  2022-01-01 01:01:01  None\n",
      "4      00005  USB charger     50  2022-01-01 01:01:01  None\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(full_load_filepath).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to s3://sb-test-bucket-ireland/soumaya/de-usecases/pandas/pandas-pyspark/bulk_insert.parquet in 0.13838529586791992\n",
      "  product_id product_name  price  ...      start_datetime end_datetime is_current\n",
      "0      00001       Heater    250  ... 2022-01-01 01:01:01   2250-01-01       True\n",
      "1      00002   Thermostat    400  ... 2022-01-01 01:01:01   2250-01-01       True\n",
      "2      00003   Television    600  ... 2022-01-01 01:01:01   2250-01-01       True\n",
      "3      00004      Blender    100  ... 2022-01-01 01:01:01   2250-01-01       True\n",
      "4      00005  USB charger     50  ... 2022-01-01 01:01:01   2250-01-01       True\n",
      "\n",
      "[5 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "bulk_insert_filepath = bulk_insert(full_load_filepath,output_data_directory,future_end_datetime)\n",
    "pd.read_parquet(bulk_insert_filepath).head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slowly Changing Dimension Type 2 - Simple\n",
    "\n",
    "This is simplified SCD2 process which does not take into account deletes.\n",
    "\n",
    "1. Join full load with updates on primary key\n",
    "2. Set `end_datetime` to the `extraction_timestamp` of the updated records \n",
    "3. Close the existing records\n",
    "4. Add the SCD2 columms to updates\n",
    "5. Append updated data to existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product_id product_name  price extraction_timestamp op\n",
      "0      00001       Heater   1000  2023-01-01 01:01:01  U\n",
      "1      00002   Thermostat   1000  2023-01-01 01:01:01  U\n",
      "2      00003   Television   1000  2023-01-01 01:01:01  U\n",
      "3      00004      Blender   1000  2023-01-01 01:01:01  U\n",
      "4      00005  USB charger   1000  2023-01-01 01:01:01  U\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(updates_filepath).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to s3://sb-test-bucket-ireland/soumaya/de-usecases/pandas/pandas-pyspark/scd2_simple.parquet in 0.2599973678588867\n",
      "  product_id product_name  ...        end_datetime is_current\n",
      "0      00001       Heater  ... 2023-01-01 01:01:01      False\n",
      "1      00002   Thermostat  ... 2023-01-01 01:01:01      False\n",
      "2      00003   Television  ... 2023-01-01 01:01:01      False\n",
      "3      00004      Blender  ... 2023-01-01 01:01:01      False\n",
      "4      00005  USB charger  ... 2023-01-01 01:01:01      False\n",
      "5      00001       Heater  ... 2250-01-01 00:00:00       True\n",
      "6      00002   Thermostat  ... 2250-01-01 00:00:00       True\n",
      "7      00003   Television  ... 2250-01-01 00:00:00       True\n",
      "8      00004      Blender  ... 2250-01-01 00:00:00       True\n",
      "9      00005  USB charger  ... 2250-01-01 00:00:00       True\n",
      "\n",
      "[10 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "scd2_simple_filepath = scd2_simple(bulk_insert_filepath,updates_filepath,output_data_directory,future_end_datetime)\n",
    "pd.read_parquet(scd2_simple_filepath).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute deleted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slowly Changing Dimension Type 2 - Complex\n",
    "\n",
    "This is a more complex SCD2 process which takes into account:\n",
    "\n",
    "- Late arriving records where an update is processed with an extraction_timestamp that is later than the extraction_timestamp of the last processed record\n",
    "- Batches which contain multiple updates to the same primary key\n",
    "\n",
    "The process can be summarised as follows:\n",
    "\n",
    "1. Concat/union updates with the existing data\n",
    "2. Sort by primary key and extraction_timestamp\n",
    "3. Window by primary key and set the end_datetime to the next record's extraction_timestamp, otherwise set it to a future distant timestamp\n",
    "\n",
    "The process could be optimised by separating records which have not received any updates, but this is left out to make the logic easier to follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product_id product_name  price extraction_timestamp op\n",
      "0      00001       Heater    500  2022-06-01 01:01:01  U\n",
      "1      00002   Thermostat    500  2022-06-01 01:01:01  U\n",
      "2      00003   Television    500  2022-06-01 01:01:01  U\n",
      "3      00004      Blender    500  2022-06-01 01:01:01  U\n",
      "4      00005  USB charger    500  2022-06-01 01:01:01  U\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(late_updates_filepath).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to s3://sb-test-bucket-ireland/soumaya/de-usecases/pandas/pandas-pyspark/scd2_complex.parquet in 0.2845125198364258\n",
      "   product_id product_name  ...        end_datetime is_current\n",
      "0       00001       Heater  ... 2022-06-01 01:01:01      False\n",
      "1       00001       Heater  ... 2023-01-01 01:01:01      False\n",
      "2       00001       Heater  ... 2250-01-01 00:00:00       True\n",
      "3       00002   Thermostat  ... 2022-06-01 01:01:01      False\n",
      "4       00002   Thermostat  ... 2023-01-01 01:01:01      False\n",
      "5       00002   Thermostat  ... 2250-01-01 00:00:00       True\n",
      "6       00003   Television  ... 2022-06-01 01:01:01      False\n",
      "7       00003   Television  ... 2023-01-01 01:01:01      False\n",
      "8       00003   Television  ... 2250-01-01 00:00:00       True\n",
      "9       00004      Blender  ... 2022-06-01 01:01:01      False\n",
      "10      00004      Blender  ... 2023-01-01 01:01:01      False\n",
      "11      00004      Blender  ... 2250-01-01 00:00:00       True\n",
      "12      00005  USB charger  ... 2022-06-01 01:01:01      False\n",
      "13      00005  USB charger  ... 2023-01-01 01:01:01      False\n",
      "14      00005  USB charger  ... 2250-01-01 00:00:00       True\n",
      "\n",
      "[15 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "scd2_complex_filepath = scd2_complex(scd2_simple_filepath,late_updates_filepath,output_data_directory,future_end_datetime)\n",
    "pd.read_parquet(scd2_complex_filepath).head(20).sort_values(by=[\"product_id\", \"extraction_timestamp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
