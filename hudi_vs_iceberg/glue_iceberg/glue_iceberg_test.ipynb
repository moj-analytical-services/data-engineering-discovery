{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iam_role is arn:aws:iam::684969100054:role/aws-reserved/sso.amazonaws.com/eu-west-2/AWSReservedSSO_AdministratorAccess_ab408ccf26c25b37\n",
      "iam_role has been set to arn:aws:iam::684969100054:role/AdminAccessGlueNotebook.\n",
      "Previous region: eu-west-1\n",
      "Setting new region to: eu-west-1\n",
      "Reauthenticating Glue client with new region: eu-west-1\n",
      "IAM role has been set to arn:aws:iam::684969100054:role/AdminAccessGlueNotebook. Reauthenticating.\n",
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\n",
      "Authentication done.\n",
      "Region is set to: eu-west-1\n",
      "Setting session ID prefix to test-\n",
      "Setting Glue version to: 3.0\n",
      "Current idle_timeout is 2880 minutes.\n",
      "idle_timeout has been set to 60 minutes.\n",
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: 5\n",
      "Setting new number of workers to: 2\n",
      "The following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\n",
    "%region eu-west-1\n",
    "%session_id_prefix test-\n",
    "%glue_version 3.0\n",
    "%idle_timeout 60\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "%%configure \n",
    "{\n",
    "  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "  \"--datalake-formats\": \"iceberg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: a104ef4d-3d12-4ae6-b78e-656f0b803759\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.37.4\n",
      "--enable-glue-datacatalog true\n",
      "--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "--datalake-formats iceberg\n",
      "Waiting for session a104ef4d-3d12-4ae6-b78e-656f0b803759 to get into ready status...\n",
      "Session a104ef4d-3d12-4ae6-b78e-656f0b803759 has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "catalog_name = \"glue_catalog\"\n",
    "bucket_name = \"sb-test-bucket-ireland\"\n",
    "bucket_prefix = \"sb\"\n",
    "table_name = \"datagensb\"\n",
    "warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"\n",
    "input_prefix = \"tpcds_test\"\n",
    "input_path = f\"s3://{bucket_name}/{input_prefix}\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database tpcds_test_glue_iceberg already exist\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_database_name = \"tpcds_test\"\n",
    "dest_database_name = \"tpcds_test_glue_iceberg\"\n",
    "output_directory = f\"{catalog_name}.{dest_database_name}.{table_name}\"\n",
    "future_end_datetime = datetime.datetime(2250, 1, 1)\n",
    "\n",
    "## Create a database with the name hudi_df to host hudi tables if not exists.\n",
    "try:\n",
    "    glue = boto3.client('glue')\n",
    "    glue.create_database(DatabaseInput={'Name': dest_database_name})\n",
    "    print(f\"New database {dest_database_name} created\")\n",
    "except glue.exceptions.AlreadyExistsException:\n",
    "    print(f\"Database {dest_database_name} already exist\")\n",
    "\n",
    "## Delete files in S3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.objects.filter(Prefix=f\"{bucket_name}/{bucket_prefix}/\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+\n",
      "|ss_sold_date_sk|ss_sold_time_sk|ss_item_sk|ss_customer_sk|ss_cdemo_sk|ss_hdemo_sk|ss_store_sk|ss_addr_sk|ss_promo_sk|ss_ticket_number|ss_quantity|ss_wholesale_cost|ss_list_price|ss_sales_price|ss_ext_discount_amt|ss_ext_sales_price|ss_ext_wholesale_cost|ss_ext_list_price|ss_ext_tax|ss_coupon_amt|ss_net_paid|ss_net_paid_inc_tax|ss_net_profit|extraction_timestamp|  op| pk|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|null|  A|\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|null|  B|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+\n"
     ]
    }
   ],
   "source": [
    "full_load = spark.read.option('header','true').parquet(f\"{input_path}/full_load\")\n",
    "full_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n",
      "|ss_sold_date_sk|ss_sold_time_sk|ss_item_sk|ss_customer_sk|ss_cdemo_sk|ss_hdemo_sk|ss_store_sk|ss_addr_sk|ss_promo_sk|ss_ticket_number|ss_quantity|ss_wholesale_cost|ss_list_price|ss_sales_price|ss_ext_discount_amt|ss_ext_sales_price|ss_ext_wholesale_cost|ss_ext_list_price|ss_ext_tax|ss_coupon_amt|ss_net_paid|ss_net_paid_inc_tax|ss_net_profit|extraction_timestamp|  op| pk|     start_datetime|       end_datetime|is_current|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|None|  A|2022-01-01 00:00:00|2250-01-01 00:00:00|      true|\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|None|  B|2022-01-01 00:00:00|2250-01-01 00:00:00|      true|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "## Drop table in Glue Data Catalog\n",
    "try:\n",
    "    glue = boto3.client('glue')\n",
    "    glue.delete_table(DatabaseName=dest_database_name, Name=table_name)\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print(f\"Table {dest_database_name}.{table_name} does not exist\")\n",
    "\n",
    "def bulk_insert(full_load_path,output_directory,future_end_datetime):\n",
    "    \n",
    "    # read the bulk insert parquet file\n",
    "    full_load=spark.read.parquet(full_load_path)\n",
    "    # adds 3 new columns\n",
    "    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
    "    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n",
    "    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n",
    "    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n",
    "    full_load.writeTo(output_directory).createOrReplace()\n",
    "    \n",
    "bulk_insert(f\"{input_path}/full_load\", output_directory,future_end_datetime)\n",
    "spark.table(output_directory).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n",
      "|ss_sold_date_sk|ss_sold_time_sk|ss_item_sk|ss_customer_sk|ss_cdemo_sk|ss_hdemo_sk|ss_store_sk|ss_addr_sk|ss_promo_sk|ss_ticket_number|ss_quantity|ss_wholesale_cost|ss_list_price|ss_sales_price|ss_ext_discount_amt|ss_ext_sales_price|ss_ext_wholesale_cost|ss_ext_list_price|ss_ext_tax|ss_coupon_amt|ss_net_paid|ss_net_paid_inc_tax|ss_net_profit|extraction_timestamp| op| pk|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          3|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-03-01 00:00:00|  U|  A|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').parquet(f\"{input_path}/cdc_1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n",
      "|ss_sold_date_sk|ss_sold_time_sk|ss_item_sk|ss_customer_sk|ss_cdemo_sk|ss_hdemo_sk|ss_store_sk|ss_addr_sk|ss_promo_sk|ss_ticket_number|ss_quantity|ss_wholesale_cost|ss_list_price|ss_sales_price|ss_ext_discount_amt|ss_ext_sales_price|ss_ext_wholesale_cost|ss_ext_list_price|ss_ext_tax|ss_coupon_amt|ss_net_paid|ss_net_paid_inc_tax|ss_net_profit|extraction_timestamp|  op| pk|     start_datetime|       end_datetime|is_current|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|None|  A|2022-01-01 00:00:00|2022-03-01 00:00:00|     false|\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          3|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-03-01 00:00:00|   U|  A|2022-03-01 00:00:00|2250-01-01 00:00:00|      true|\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          1|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-01-01 00:00:00|None|  B|2022-01-01 00:00:00|2250-01-01 00:00:00|      true|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+----+---+-------------------+-------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "def scd2_simple(updates_filepath, output_directory, future_end_datetime, primary_key):\n",
    "\n",
    "    # read the new updates parquet file\n",
    "    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n",
    "    # adds 3 new columns\n",
    "    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
    "    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n",
    "    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n",
    "    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\") \n",
    "    simple_merge_sql = f\"\"\"\n",
    "    MERGE INTO {catalog_name}.{dest_database_name}.{table_name} dest\n",
    "        USING (   \n",
    "            SELECT\n",
    "            ss_sold_date_sk,\n",
    "            ss_sold_time_sk,\n",
    "            ss_item_sk,\n",
    "            ss_customer_sk,\n",
    "            ss_cdemo_sk,\n",
    "            ss_hdemo_sk,\n",
    "            ss_addr_sk, \n",
    "            ss_store_sk,\n",
    "            ss_promo_sk,\n",
    "            ss_ticket_number,\n",
    "            ss_quantity,\n",
    "            ss_wholesale_cost,\n",
    "            ss_list_price,\n",
    "            ss_sales_price, \n",
    "            ss_ext_discount_amt,\n",
    "            ss_ext_sales_price,\n",
    "            ss_ext_wholesale_cost,\n",
    "            ss_ext_list_price,\n",
    "            ss_ext_tax,\n",
    "            ss_coupon_amt,\n",
    "            ss_net_paid, \n",
    "            ss_net_paid_inc_tax,\n",
    "            ss_net_profit, \n",
    "            extraction_timestamp, \n",
    "            op, \n",
    "            pk,\n",
    "            start_datetime, \n",
    "            end_datetime, \n",
    "            is_current\n",
    "                FROM tmp_{table_name}_updates\n",
    "        UNION ALL\n",
    "            SELECT\n",
    "            t.ss_sold_date_sk,\n",
    "            t.ss_sold_time_sk, \n",
    "            t.ss_item_sk,\n",
    "            t.ss_customer_sk,\n",
    "            t.ss_cdemo_sk,\n",
    "            t.ss_hdemo_sk,\n",
    "            t.ss_addr_sk, \n",
    "            t.ss_store_sk,\n",
    "            t.ss_promo_sk,\n",
    "            t.ss_ticket_number,\n",
    "            t.ss_quantity, \n",
    "            t.ss_wholesale_cost,\n",
    "            t.ss_list_price,\n",
    "            t.ss_sales_price, \n",
    "            t.ss_ext_discount_amt,\n",
    "            t.ss_ext_sales_price,\n",
    "            t.ss_ext_wholesale_cost, \n",
    "            t.ss_ext_list_price,\n",
    "            t.ss_ext_tax,\n",
    "            t.ss_coupon_amt,\n",
    "            t.ss_net_paid, \n",
    "            t.ss_net_paid_inc_tax,\n",
    "            t.ss_net_profit, \n",
    "            t.extraction_timestamp, \n",
    "            t.op, \n",
    "            t.pk,\n",
    "            t.start_datetime,\n",
    "            u.start_datetime AS end_datetime, \n",
    "            u.is_current\n",
    "            FROM {catalog_name}.{dest_database_name}.{table_name} as t\n",
    "            INNER JOIN tmp_{table_name}_updates as u ON t.pk = u.pk AND t.is_current = true\n",
    "        ) AS src\n",
    "        ON (dest.pk = src.pk AND dest.extraction_timestamp=src.extraction_timestamp)\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET end_datetime = src.end_datetime,is_current = false\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (\n",
    "                ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk, ss_hdemo_sk, ss_addr_sk, \n",
    "                ss_store_sk, ss_promo_sk, ss_ticket_number, ss_quantity, ss_wholesale_cost, ss_list_price, ss_sales_price, \n",
    "                ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price, ss_ext_tax, ss_coupon_amt, \n",
    "                ss_net_paid, ss_net_paid_inc_tax, ss_net_profit, extraction_timestamp, op, pk, start_datetime, end_datetime, is_current\n",
    "            )\n",
    "            VALUES (\n",
    "                src.ss_sold_date_sk, src.ss_sold_time_sk, src.ss_item_sk, src.ss_customer_sk, src.ss_cdemo_sk, src.ss_hdemo_sk, \n",
    "                src.ss_addr_sk, src.ss_store_sk, src.ss_promo_sk, src.ss_ticket_number, src.ss_quantity, src.ss_wholesale_cost,\n",
    "                src.ss_list_price, src.ss_sales_price, src.ss_ext_discount_amt, src.ss_ext_sales_price, src.ss_ext_wholesale_cost, \n",
    "                src.ss_ext_list_price, src.ss_ext_tax, src.ss_coupon_amt, src.ss_net_paid, src.ss_net_paid_inc_tax, src.ss_net_profit,\n",
    "                src.extraction_timestamp, src.op, src.pk, src.extraction_timestamp, src.end_datetime, true\n",
    "            )\n",
    "            \"\"\"   \n",
    "    spark.sql(simple_merge_sql).writeTo(output_directory)\n",
    "\n",
    "\n",
    "scd2_simple(f\"{input_path}/cdc_1\", output_directory,future_end_datetime, \"pk\")\n",
    "spark.table(output_directory).sort(\"pk\",\"extraction_timestamp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n",
      "|ss_sold_date_sk|ss_sold_time_sk|ss_item_sk|ss_customer_sk|ss_cdemo_sk|ss_hdemo_sk|ss_store_sk|ss_addr_sk|ss_promo_sk|ss_ticket_number|ss_quantity|ss_wholesale_cost|ss_list_price|ss_sales_price|ss_ext_discount_amt|ss_ext_sales_price|ss_ext_wholesale_cost|ss_ext_list_price|ss_ext_tax|ss_coupon_amt|ss_net_paid|ss_net_paid_inc_tax|ss_net_profit|extraction_timestamp| op| pk|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n",
      "|           null|           null|      null|          null|       null|       null|       null|      null|       null|            null|          4|             null|         null|          null|               null|              null|                 null|             null|      null|         null|       null|               null|         null| 2022-04-01 00:00:00|  I|  C|\n",
      "+---------------+---------------+----------+--------------+-----------+-----------+-----------+----------+-----------+----------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+-----------+-------------------+-------------+--------------------+---+---+\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').parquet(f\"{input_path}/cdc_2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: 'DataFrame' object has no attribute 'order'\n"
     ]
    }
   ],
   "source": [
    "scd2_simple(f\"{input_path}/cdc_2\", output_directory,future_end_datetime, \"pk\")\n",
    "spark.table(output_directory).sort(\"pk\",\"extraction_timestamp\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd2_complex(updates_filepath, output_directory, future_end_datetime, primary_key):\n",
    "\n",
    "    # read the new updates parquet file\n",
    "    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n",
    "    # adds 3 new columns\n",
    "    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
    "    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n",
    "    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n",
    "    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\") \n",
    "    simple_merge_sql = f\"\"\"\n",
    "    MERGE INTO {catalog_name}.{dest_database_name}.{table_name} dest\n",
    "        USING (   \n",
    "            SELECT\n",
    "            ss_sold_date_sk,\n",
    "            ss_sold_time_sk,\n",
    "            ss_item_sk,\n",
    "            ss_customer_sk,\n",
    "            ss_cdemo_sk,\n",
    "            ss_hdemo_sk,\n",
    "            ss_addr_sk, \n",
    "            ss_store_sk,\n",
    "            ss_promo_sk,\n",
    "            ss_ticket_number,\n",
    "            ss_quantity,\n",
    "            ss_wholesale_cost,\n",
    "            ss_list_price,\n",
    "            ss_sales_price, \n",
    "            ss_ext_discount_amt,\n",
    "            ss_ext_sales_price,\n",
    "            ss_ext_wholesale_cost,\n",
    "            ss_ext_list_price,\n",
    "            ss_ext_tax,\n",
    "            ss_coupon_amt,\n",
    "            ss_net_paid, \n",
    "            ss_net_paid_inc_tax,\n",
    "            ss_net_profit, \n",
    "            extraction_timestamp, \n",
    "            op, \n",
    "            pk,\n",
    "            start_datetime, \n",
    "            end_datetime, \n",
    "            is_current\n",
    "                FROM tmp_{table_name}_updates\n",
    "        UNION ALL\n",
    "            SELECT\n",
    "            t.ss_sold_date_sk,\n",
    "            t.ss_sold_time_sk, \n",
    "            t.ss_item_sk,\n",
    "            t.ss_customer_sk,\n",
    "            t.ss_cdemo_sk,\n",
    "            t.ss_hdemo_sk,\n",
    "            t.ss_addr_sk, \n",
    "            t.ss_store_sk,\n",
    "            t.ss_promo_sk,\n",
    "            t.ss_ticket_number,\n",
    "            t.ss_quantity, \n",
    "            t.ss_wholesale_cost,\n",
    "            t.ss_list_price,\n",
    "            t.ss_sales_price, \n",
    "            t.ss_ext_discount_amt,\n",
    "            t.ss_ext_sales_price,\n",
    "            t.ss_ext_wholesale_cost, \n",
    "            t.ss_ext_list_price,\n",
    "            t.ss_ext_tax,\n",
    "            t.ss_coupon_amt,\n",
    "            t.ss_net_paid, \n",
    "            t.ss_net_paid_inc_tax,\n",
    "            t.ss_net_profit, \n",
    "            t.extraction_timestamp, \n",
    "            t.op, \n",
    "            t.pk,\n",
    "            t.start_datetime,\n",
    "            u.start_datetime AS end_datetime, \n",
    "            u.is_current\n",
    "            FROM {catalog_name}.{dest_database_name}.{table_name} as t\n",
    "            INNER JOIN tmp_{table_name}_updates as u ON t.pk = u.pk AND t.is_current = true\n",
    "        ) AS src\n",
    "        ON (dest.pk = src.pk AND dest.extraction_timestamp=src.extraction_timestamp)\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET end_datetime = src.end_datetime,is_current = false\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (\n",
    "                ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk, ss_hdemo_sk, ss_addr_sk, \n",
    "                ss_store_sk, ss_promo_sk, ss_ticket_number, ss_quantity, ss_wholesale_cost, ss_list_price, ss_sales_price, \n",
    "                ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price, ss_ext_tax, ss_coupon_amt, \n",
    "                ss_net_paid, ss_net_paid_inc_tax, ss_net_profit, extraction_timestamp, op, pk, start_datetime, end_datetime, is_current\n",
    "            )\n",
    "            VALUES (\n",
    "                src.ss_sold_date_sk, src.ss_sold_time_sk, src.ss_item_sk, src.ss_customer_sk, src.ss_cdemo_sk, src.ss_hdemo_sk, \n",
    "                src.ss_addr_sk, src.ss_store_sk, src.ss_promo_sk, src.ss_ticket_number, src.ss_quantity, src.ss_wholesale_cost,\n",
    "                src.ss_list_price, src.ss_sales_price, src.ss_ext_discount_amt, src.ss_ext_sales_price, src.ss_ext_wholesale_cost, \n",
    "                src.ss_ext_list_price, src.ss_ext_tax, src.ss_coupon_amt, src.ss_net_paid, src.ss_net_paid_inc_tax, src.ss_net_profit,\n",
    "                src.extraction_timestamp, src.op, src.pk, src.extraction_timestamp, src.end_datetime, true\n",
    "            )\n",
    "            \"\"\"   \n",
    "    spark.sql(simple_merge_sql).writeTo(output_directory)\n",
    "\n",
    "\n",
    "scd2_simple(f\"{input_path}/cdc_1\", output_directory,future_end_datetime, \"pk\")\n",
    "spark.table(output_directory).sort(\"pk\",\"extraction_timestamp\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
