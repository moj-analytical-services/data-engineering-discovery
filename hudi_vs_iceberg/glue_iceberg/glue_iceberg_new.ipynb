{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Glue + Iceberg evaluation",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "###### Bulk Insert \n###### SCD2\n###### Impute deletions\n###### Deduplication\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Initialise SparkSession",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Clean up existing resources ",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Create Iceberg tables, insert synthetic data (25K rows)",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Bulk Insert \n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "Setting session ID prefix to native-iceberg-dataframe-\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 3305037b-2670-4661-9e2d-dbeab9788877.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 3305037b-2670-4661-9e2d-dbeab9788877.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 60 minutes.\nidle_timeout has been set to 60 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 3305037b-2670-4661-9e2d-dbeab9788877.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"sb-test-bucket-ireland\"\nbucket_prefix = \"sb\"\ndatabase_name = \"sb10_iceberg_dataframe\"\ntable_name = \"datagensb\"\nwarehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\noutput_directory = f\"{catalog_name}.{database_name}.{table_name}\"\nfuture_end_datetime = \"2050-01-01\"\nprimary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load=spark.read.option('header','true').parquet(input_filepath)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, Row\nimport time\n\n\ndef bulk_insert(input_filepath,output_directory,future_end_datetime):\n    start = time.time()\n    full_load=spark.read.option('header','true').parquet(input_filepath)\n    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n    full_load.writeTo(output_directory) \\\n    .create()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "bulk_insert(input_filepath,output_directory,future_end_datetime)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "10.359502792358398\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "output_directory",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "'glue_catalog.sb9_iceberg_dataframe.datagensb'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-------------------+---------+-------------------+\n|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n+--------------------+-------------------+---------+-------------------+\n|2023-05-31 15:14:...|9122969471621716419|     null|               true|\n+--------------------+-------------------+---------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Slowly Changing Dimension Type 2 (SCD2)\nThe updates are created by replacing one column with the same value to simplify the testing. The soft deletes are not taken into account since very similar process from a performance perspective.\n\nSteps:\n\nRead updates\nJoin full load with updates on primary key\nSet end_datetime to the extraction_timestamp of the updated records\nClose the existing records\nAdd curation columms to updates\nAppend updated data to existing data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(\"2050-01-01\"), 'yyyy-MM-dd'))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#full_load_updates = full_load_updates.limit(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.schema",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType(List(StructField(product_id,StringType,true),StructField(product_name,StringType,true),StructField(price,LongType,true),StructField(extraction_timestamp,TimestampType,true),StructField(op,StringType,true),StructField(start_datetime,TimestampType,true),StructField(end_datetime,TimestampType,true),StructField(is_current,BooleanType,false)))\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nfull_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nMERGE INTO {catalog_name}.{database_name}.{table_name} AS f\nUSING (SELECT * FROM tmp_{table_name}_updates) AS u\nON f.{primary_key} = u.{primary_key}\nWHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "primary_key",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "'product_id'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key):\n    start = time.time()\n    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n\n    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")\n    query = f\"\"\"\n    MERGE INTO {catalog_name}.{database_name}.{table_name} AS f\n    USING (SELECT * FROM tmp_{table_name}_updates) AS u\n    ON f.{primary_key} = u.{primary_key}\n    WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n    \"\"\"\n    spark.sql(query)\n    full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()\n    print(time.time()-start)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Slowly Changing Dimension Type 2 - Complex\nThis is a more complex SCD2 process which takes into account:\nLate arriving records where an update is processed with an extraction_timestamp that is later than the extraction_timestamp of the last processed record\nBatches which contain multiple updates to the same primary key\nThe process can be summarised as follows:\n\nConcat/union updates with the existing data\nSort by primary key and extraction_timestamp\nWindow by primary key and set the end_datetime to the next record's extraction_timestamp, otherwise set it to a future distant timestamp\nThe process could be optimised by separating records which have not received any updates, but this is left out to make the logic easier to follow.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#late_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "late_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\")\nlate_updates = late_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\nlate_updates = late_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(\"2050-01-01\"), 'yyyy-MM-dd'))\nlate_updates = late_updates.withColumn(\"is_current\",F.lit(True))\nprimary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#primary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#late_updates = late_updates.limit(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "late_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00001|      Heater|  500| 2022-06-01 01:01:01|  U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|  U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  500| 2022-06-01 01:01:01|  U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|  U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|  U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|\n+----------+------------+-----+--------------------+----+-------------------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n+----------+------------+-----+--------------------+----+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#late_updates.createOrReplaceTempView(f\"tmp_{table_name}_late_updates\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "late_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").drop(\"end_datetime\",\"is_current\").writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").drop(\"end_datetime\",\"is_current\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#spark.sql(query1).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Found duplicate column(s) in the table definition of sb8_iceberg_dataframe.datagensb: `end_datetime`\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|\n+----------+------------+-----+--------------------+----+-------------------+\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|\n+----------+------------+-----+--------------------+----+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#spark.table(output_directory).writeTo(output_directory) .create()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query1 = f\"\"\"\nSELECT *,\nLEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\nspark.sql(query1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp, end_datetime: timestamp]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query2 = f\"\"\"\nSELECT *,\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\nspark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp, end_datetime: timestamp, is_current: boolean]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.table(output_directory).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## test code",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# query2 = f\"\"\"\n\n# ALTER TABLE {catalog_name}.{database_name}.{table_name}\n# DROP COLUMN is_current\n\n\n#     \"\"\"\n# spark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query1 = f\"\"\"\nSELECT *,\nLEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\nspark.sql(query1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp, end_datetime: timestamp, end_datetime: timestamp]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query1 = f\"\"\"\nALTER TABLE {catalog_name}.{database_name}.{table_name}\nALTER end_datetime\nSELECT *,\nLEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp)\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\nspark.sql(query1)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query1 = f\"\"\"\nCREATE TEMPORARY TABLE temp SELECT *  \nFROM {catalog_name}.{database_name}.{table_name};\n\nALTER TABLE temp DROP (end_datetime);\nSELECT *,\nLEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) END AS end_datetime\n\nFROM temp\n\nORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\nspark.sql(query1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "ParseException: \nextraneous input 'ALTER' expecting {<EOF>, ';'}(line 5, pos 0)\n\n== SQL ==\n\nCREATE TEMPORARY TABLE temp SELECT *  \nFROM glue_catalog.sb10_iceberg_dataframe.datagensb;\n\nALTER TABLE temp DROP (end_datetime);\n^^^\nSELECT *,\nLEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY product_id ORDER BY extraction_timestamp) END AS end_datetime\n\nFROM temp\n\nORDER BY product_id, extraction_timestamp\n    \n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# query1 = f\"\"\"\n# MERGE INTO {catalog_name}.{database_name}.{table_name} AS f\n# USING (SELECT * FROM tmp_{table_name}_late_updates) AS u\n# ON f.{primary_key} = u.{primary_key}\n# WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False  \n#     \"\"\"\n# spark.sql(query1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Cannot delete missing field is_current in glue_catalog.sb8_iceberg_dataframe.datagensb schema: root\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- price: long (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = true)\n |-- op: string (nullable = true)\n |-- start_datetime: timestamp (nullable = true)\n |-- end_datetime: timestamp (nullable = true)\n; line 3 pos 0;\n'AlterTable org.apache.iceberg.spark.SparkCatalog@45000dd6, sb8_iceberg_dataframe.datagensb, RelationV2[product_id#1166, product_name#1167, price#1168L, extraction_timestamp#1169, op#1170, start_datetime#1171, end_datetime#1172] glue_catalog.sb8_iceberg_dataframe.datagensb, [org.apache.spark.sql.connector.catalog.TableChange$DeleteColumn@9c98a783]\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "MERGE INTO {catalog_name}.{database_name}.{table_name} AS f",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "late_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 70,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# query1 = f\"\"\"\n# SELECT *,\n# LEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n\n# FROM {catalog_name}.{database_name}.{table_name}\n\n# ORDER BY {primary_key}, extraction_timestamp\n#     \"\"\"\n# spark.sql(query1)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query2 = f\"\"\"\nSELECT * EXCEPT[is_current]\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY product_id, extraction_timestamp\n    \"\"\"\nspark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "ParseException: \nmismatched input '[' expecting {<EOF>, ';'}(line 2, pos 15)\n\n== SQL ==\n\nSELECT * EXCEPT[is_current]\n---------------^^^\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\nFROM glue_catalog.sb7_iceberg_dataframe.datagensb\n\nORDER BY product_id, extraction_timestamp\n    \n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query2 = f\"\"\"\nCREATE TEMPORARY TABLE temp SELECT *  \nFROM {catalog_name}.{database_name}.{table_name},\nALTER TABLE temp DROP (is_current),\nSELECT * FROM temptable\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\nSELECT * FROM temptable\n\n    \"\"\"\nspark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 69,
			"outputs": [
				{
					"name": "stdout",
					"text": "ParseException: \nmismatched input 'temp' expecting {<EOF>, ';'}(line 4, pos 12)\n\n== SQL ==\n\nCREATE TEMPORARY TABLE temp SELECT *  \nFROM glue_catalog.sb7_iceberg_dataframe.datagensb,\nALTER TABLE temp DROP (is_current),\n------------^^^\nSELECT * FROM temptable\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\nSELECT * FROM temptable\n\n    \n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query2 = f\"\"\"\n\nSELECT * INTO temptable \nFROM {catalog_name}.{database_name}.{table_name}\nALTER TABLE temptable\nDROP COLUMN is_current \nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\nSELECT * FROM temptable\nDROP TABLE temptable\n    \"\"\"\nspark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "ParseException: \nmismatched input 'temptable' expecting {<EOF>, ';'}(line 3, pos 14)\n\n== SQL ==\n\n\nSELECT * INTO temptable \n--------------^^^\nFROM glue_catalog.sb7_iceberg_dataframe.datagensb\nALTER TABLE temptable\nDROP COLUMN is_current \nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\nSELECT * FROM temptable\nDROP TABLE temptable\n    \n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Found duplicate column(s) in the table definition of sb2_iceberg_dataframe.datagensb: `end_datetime`\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+-------------------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|       end_datetime|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+-------------------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|2022-06-01 01:01:01|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|2050-01-01 00:00:00|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|2022-06-01 01:01:01|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|2050-01-01 00:00:00|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|2022-06-01 01:01:01|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|2050-01-01 00:00:00|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|2022-06-01 01:01:01|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|2050-01-01 00:00:00|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|2022-06-01 01:01:01|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|2050-01-01 00:00:00|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# query2 = f\"\"\"\n# SELECT *,\n# CASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\n# FROM {catalog_name}.{database_name}.{table_name}\n\n# ORDER BY {primary_key}, extraction_timestamp\n#     \"\"\"\n# spark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp, end_datetime: timestamp, is_current: boolean, is_current: boolean]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Found duplicate column(s) in the table definition of sb4_iceberg_dataframe.datagensb: `is_current`\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key):\n    start = time.time()\n    late_updates = spark.read.option('header','true').parquet(late_updates_filepath)\n    late_updates = late_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    late_updates.writeTo(output_directory).append()\n    spark.table(f\"{catalog_name}.{database_name}.{table_name}\").drop(\"end_datetime\",\"is_current\").writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()\n    query1 = f\"\"\"\n    SELECT *,\n    LEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n\n    FROM {catalog_name}.{database_name}.{table_name}\n\n    ORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\n    spark.sql(query1)\n    spark.sql(query1).writeTo(output_directory).createOrReplace()\n    query2 = f\"\"\"\n    SELECT *,\n    CASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\n    FROM {catalog_name}.{database_name}.{table_name}\n\n    ORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\n    spark.sql(query2)\n    spark.sql(query2).writeTo(output_directory).createOrReplace()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "late_updates_filepath=\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "primary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Cannot write incompatible data to table 'glue_catalog.sb8_iceberg_dataframe.datagensb':\n- Cannot find data for output column 'end_datetime'\n- Cannot find data for output column 'is_current'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query1 = f\"\"\"\nSELECT product_id, product_name, price, extraction_timestamp, op, start_datetime,is_current,\nLEAD(start_datetime,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY product_id ORDER BY start_datetime) AS end_datetime\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY product_id, extraction_timestamp\n    \"\"\"\nspark.sql(query1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").overwritePartitions()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query1).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query2 = f\"\"\"\nSELECT product_id, product_name, price, extraction_timestamp, op, start_datetime,end_datetime,\nCASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\nFROM {catalog_name}.{database_name}.{table_name}\n\nORDER BY product_id, extraction_timestamp\n    \"\"\"\nspark.sql(query2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).writeTo(f\"{catalog_name}.{database_name}.{table_name}\").overwritePartitions()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(query2).show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nSetting session ID prefix to native-iceberg-dataframe-\nSetting Glue version to: 3.0\nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"sb-test-bucket-ireland\"\nbucket_prefix = \"sb\"\ndatabase_name = \"sb13_iceberg_dataframe\"\ntable_name = \"datagensb\"\nwarehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\noutput_directory = f\"{catalog_name}.{database_name}.{table_name}\"\nfuture_end_datetime = \"2050-01-01\"\nupdates_filepath =\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\"\nprimary_key = \"product_id\"\nlate_updates_filepath=\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Functions",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, Row\nimport time\n\n\ndef bulk_insert(input_filepath,output_directory,future_end_datetime):\n    start = time.time()\n    full_load=spark.read.option('header','true').parquet(input_filepath)\n    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n    full_load.sortWithinPartitions(\"product_name\") \\\n    .writeTo(output_directory) \\\n    .create()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "bulk_insert(input_filepath,output_directory,future_end_datetime)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "3.3868606090545654\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# updates_filepath =\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\"\n# primary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key):\n    start = time.time()\n    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n\n    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")\n    query = f\"\"\"\n    MERGE INTO {catalog_name}.{database_name}.{table_name} AS f\n    USING (SELECT * FROM tmp_{table_name}_updates) AS u\n    ON f.{primary_key} = u.{primary_key}\n    WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n    \"\"\"\n    spark.sql(query)\n    full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "11.610728025436401\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\n# output_directory = f\"{catalog_name}.{database_name}.{table_name}\"\n# future_end_datetime = \"2050-01-01\"\n# late_updates_filepath=\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\"",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").drop(\"end_datetime\",\"is_current\").writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key):\n    start = time.time()\n    late_updates = spark.read.option('header','true').parquet(late_updates_filepath)\n    late_updates = late_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    late_updates.writeTo(output_directory).append()\n    spark.table(output_directory).drop(\"end_datetime\",\"is_current\").writeTo(output_directory).createOrReplace()\n    #spark.table(output_directory).writeTo(output_directory).drop(\"end_datetime\",\"is_current\").createOrReplace()\n    query1 = f\"\"\"\n    SELECT *,\n    LEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n\n    FROM {catalog_name}.{database_name}.{table_name}\n\n    ORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\n    spark.sql(query1)\n    spark.sql(query1).writeTo(output_directory).createOrReplace()\n    query2 = f\"\"\"\n    SELECT *,\n    CASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n\n    FROM {catalog_name}.{database_name}.{table_name}\n\n    ORDER BY {primary_key}, extraction_timestamp\n    \"\"\"\n    spark.sql(query2)\n    spark.sql(query2).writeTo(output_directory).createOrReplace()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "7.640847444534302\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}