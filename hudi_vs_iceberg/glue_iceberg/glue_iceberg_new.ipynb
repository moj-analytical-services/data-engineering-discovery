{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true
			},
			"source": [
				"## Glue + Iceberg evaluation"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"###### Bulk Insert \n",
				"###### SCD2\n",
				"###### Impute deletions\n",
				"###### Deduplication\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 0.37.3 \n",
						"Setting session ID prefix to native-iceberg-dataframe-\n",
						"Setting Glue version to: 3.0\n",
						"Current idle_timeout is 2800 minutes.\n",
						"idle_timeout has been set to 60 minutes.\n",
						"The following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n"
					]
				}
			],
			"source": [
				"%session_id_prefix native-iceberg-dataframe-\n",
				"%glue_version 3.0\n",
				"%idle_timeout 60\n",
				"%%configure \n",
				"{\n",
				"  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
				"  \"--datalake-formats\": \"iceberg\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 22,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"catalog_name = \"glue_catalog\"\n",
				"bucket_name = \"sb-test-bucket-ireland\"\n",
				"bucket_prefix = \"sb\"\n",
				"database_name = \"sb13_iceberg_dataframe\"\n",
				"table_name = \"datagensb\"\n",
				"warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": 23,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from pyspark.sql import SparkSession\n",
				"spark = SparkSession.builder \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
				"    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
				"    .getOrCreate()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 24,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[]\n"
					]
				}
			],
			"source": [
				"query = f\"\"\"\n",
				"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\n",
				"\"\"\"\n",
				"spark.sql(query)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": 25,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\n",
				"output_directory = f\"{catalog_name}.{database_name}.{table_name}\"\n",
				"future_end_datetime = \"2050-01-01\"\n",
				"updates_filepath =\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\"\n",
				"primary_key = \"product_id\"\n",
				"late_updates_filepath=\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Functions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 26,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from pyspark.sql import SparkSession, functions as F\n",
				"from pyspark.sql.types import StructType, StructField, IntegerType, Row\n",
				"import time\n",
				"\n",
				"\n",
				"def bulk_insert(input_filepath,output_directory,future_end_datetime):\n",
				"    start = time.time()\n",
				"    full_load=spark.read.option('header','true').parquet(input_filepath)\n",
				"    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
				"    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n",
				"    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n",
				"    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n",
				"    full_load.sortWithinPartitions(\"product_name\") \\\n",
				"    .writeTo(output_directory) \\\n",
				"    .create()\n",
				"    print(time.time()-start)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 27,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"3.3868606090545654\n"
					]
				}
			],
			"source": [
				"bulk_insert(input_filepath,output_directory,future_end_datetime)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 28,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n"
					]
				}
			],
			"source": [
				"spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# updates_filepath =\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\"\n",
				"# primary_key = \"product_id\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": 29,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key):\n",
				"    start = time.time()\n",
				"    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n",
				"    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
				"    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n",
				"    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n",
				"\n",
				"    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")\n",
				"    query = f\"\"\"\n",
				"    MERGE INTO {catalog_name}.{database_name}.{table_name} AS f\n",
				"    USING (SELECT * FROM tmp_{table_name}_updates) AS u\n",
				"    ON f.{primary_key} = u.{primary_key}\n",
				"    WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n",
				"\n",
				"    \"\"\"\n",
				"    spark.sql(query)\n",
				"    full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()\n",
				"    print(time.time()-start)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 30,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"11.610728025436401\n"
					]
				}
			],
			"source": [
				"scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 31,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n"
					]
				}
			],
			"source": [
				"spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\n",
				"# output_directory = f\"{catalog_name}.{database_name}.{table_name}\"\n",
				"# future_end_datetime = \"2050-01-01\"\n",
				"# late_updates_filepath=\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": 32,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"spark.table(f\"{catalog_name}.{database_name}.{table_name}\").drop(\"end_datetime\",\"is_current\").writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 33,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key):\n",
				"    start = time.time()\n",
				"    late_updates = spark.read.option('header','true').parquet(late_updates_filepath)\n",
				"    late_updates = late_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n",
				"    late_updates.writeTo(output_directory).append()\n",
				"    spark.table(output_directory).drop(\"end_datetime\",\"is_current\").writeTo(output_directory).createOrReplace()\n",
				"    #spark.table(output_directory).writeTo(output_directory).drop(\"end_datetime\",\"is_current\").createOrReplace()\n",
				"    query1 = f\"\"\"\n",
				"    SELECT *,\n",
				"    LEAD(extraction_timestamp,1,TO_TIMESTAMP('2050-01-01 00:00:00')) OVER(PARTITION BY {primary_key} ORDER BY extraction_timestamp) AS end_datetime\n",
				"\n",
				"    FROM {catalog_name}.{database_name}.{table_name}\n",
				"\n",
				"    ORDER BY {primary_key}, extraction_timestamp\n",
				"    \"\"\"\n",
				"    spark.sql(query1)\n",
				"    spark.sql(query1).writeTo(output_directory).createOrReplace()\n",
				"    query2 = f\"\"\"\n",
				"    SELECT *,\n",
				"    CASE WHEN end_datetime = '2050-01-01 00:00:00' THEN True ELSE False END AS is_current\n",
				"\n",
				"    FROM {catalog_name}.{database_name}.{table_name}\n",
				"\n",
				"    ORDER BY {primary_key}, extraction_timestamp\n",
				"    \"\"\"\n",
				"    spark.sql(query2)\n",
				"    spark.sql(query2).writeTo(output_directory).createOrReplace()\n",
				"    print(time.time()-start)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 34,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"7.640847444534302\n"
					]
				}
			],
			"source": [
				"scd2_complex(input_filepath, late_updates_filepath, output_directory, primary_key)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 35,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
						"|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n",
						"|     00001|      Heater|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n",
						"|     00002|  Thermostat|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n",
						"|     00003|  Television|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n",
						"|     00004|     Blender|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2022-06-01 01:01:01|     false|\n",
						"|     00005| USB charger|  500| 2022-06-01 01:01:01|   U|2022-06-01 01:01:01|2023-01-01 01:01:01|     false|\n",
						"|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n",
						"+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n"
					]
				}
			],
			"source": [
				"spark.table(f\"{catalog_name}.{database_name}.{table_name}\").show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
